---
title: "431 Class 14"
author: "thomaselove.github.io/431"
date: "2020-10-08"
output:
  beamer_presentation:
    theme: "Madrid"
    fonttheme: "structurebold"
    colortheme: "whale"
    fig_height: 5.5
    fig_caption: false
---

```{r set-options, echo=FALSE, cache=FALSE, message = FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 55)
```

## Today's Agenda

- p values and Researcher Degrees of Freedom
  - The "Garden of Forking Paths"
- Comparing Means using Two Independent Samples
  - Regression models to obtain pooled t comparisons
  - Welch's t test (not assuming equal population variances)
  - Bootstrap methods for comparing means in 2 samples
  - Rank-based alternatives (Wilcoxon-Mann-Whitney)
- Comparing More than Two Means with ANOVA
  - using regression to compare more than two population means

## Today's R Packages and Data

```{r, message = FALSE}
library(broom)
library(janitor)
library(knitr)
library(magrittr)
library(patchwork)
library(tidyverse)

theme_set(theme_bw())

source("data/Love-boost.R") # new today!

dm431 <- readRDS("data/dm431_2020.Rds")
```

# p Hacking and "Researcher Degrees of Freedom"

## p values?

**Question 1**. If the *p* value is smaller than our pre-specified $\alpha$ level, then we can declare the results to be statistically significant and celebrate?

Well, no.


**Question 2**. What if the *p* value is greater than our $\alpha$, say *p > 0.05*? Then what?

---

![](images/bear.jpg)

## What is a *p* value?

> The probability of getting results at least as extreme as the ones you observed, given that the null hypothesis is correct.

It's a conditional probability statement. That's all.

> We want to know if results are right, but a p-value doesn't measure that. It can't tell you the magnitude of an effect, the strength of the evidence or the probability that the finding was the result of chance.

Quotes from Christie Aschwanden "[Not Even Scientists Can Easily Explain P-values](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)", at FiveThirtyEight.com on 2015-11-24

## "Researcher Degrees of Freedom", 1

> [I]t is unacceptably easy to publish "statistically significant" evidence consistent with any hypothesis.

> The culprit is a construct we refer to as **researcher degrees of freedom**. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?

Simmons et al. [$\textcolor{blue}{link}$](http://journals.sagepub.com/doi/abs/10.1177/0956797611417632) 

## "Researcher Degrees of Freedom", 2

> ... It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields statistical significance, and to then report only what worked. The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level is necessarily greater than 5%.

For more, see 

- Gelman's blog [$\textcolor{blue}{2012-11-01}$](http://andrewgelman.com/2012/11/01/researcher-degrees-of-freedom/) "Researcher Degrees of Freedom", 
- Paper by [$\textcolor{blue}{Simmons}$](http://journals.sagepub.com/doi/abs/10.1177/0956797611417632) and others, defining the term.

## Hack Your Way To Scientific Glory

https://fivethirtyeight.com/features/science-isnt-broken

![](images/hacking.png)

## How did you do?

What kind of results did you obtain?

## What can you get?

In just a few minutes, I was able to get 

- *p* < 0.01 (positive effect of Democrats on economy)
- *p* = 0.01 (negative effect of Democrats)
- *p* = 0.03 (negative effect of Democrats)
- *p* = 0.03 (positive effect of Democrats)

but also ...

- *p* = 0.05, 0.06, 0.07, 0.09, 0.17, 0.19, 0.20, 0.22, 0.23, 0.47, 0.51

without even switching parties, exclusively by changing my definitions of terms (section 2 of the graphic.)

## And this is really hard to deal with...

**The garden of forking paths**: Why multiple comparisons can be a problem, even when there is no fishing expedition or p-hacking and the research hypothesis was posited ahead of time

> Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings
where researchers perform only a single analysis on their data. The problem is there can be a
large number of potential comparisons when the details of data analysis are highly contingent on
data, without the researcher having to perform any conscious procedure of fishing or examining
multiple p-values. We discuss in the context of several examples of published papers where
data-analysis decisions were theoretically-motivated based on previous literature, but where the
details of data selection and analysis were not pre-specified and, as a result, were contingent on
data.

- [$\textcolor{blue}{Link}$](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf) to the paper from Gelman and Loken

# Comparing Population Means using Two Independent Samples

## Two Examples Comparing Two Means with `dm431`

Our population: ALL adults ages 31-70 seen for care this year and two years ago who live in Northeast Ohio with a diabetes diagnosis.

Our `dm431` sample: 431 of those people, drawn in a way we hope is representative (but certainly isn't random).

1. Can we estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not?

2. Can we estimate the difference between females and males in terms of the population mean systolic blood pressure?

### Today's Plan

1. We'll walk through example 1 (the harder example, as it turns out.) 
2. Example 2 slides follow Example 1, for you to review on your own.

## `dm431` Example 1 (Comparing LDL by Statin usage)

Estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not.

```{r, message = FALSE}
mosaic::favstats(ldl ~ statin, data = dm431) %>% 
  kable(digits = 2)
```

>- What is the outcome here? 
>- What are the two exposure groups we are comparing?
>- What are the sample means, $\bar{x}_{Statin}$ and $\bar{x}_{No Statin}$?
>- How might we estimate the difference in population means, $\mu_S - \mu_N$?
>- Is there a problem in these data we need to deal with?

## How much missing data do we have?

Do we have missing values in both columns, or just one?

```{r}
dm431 %>% summarize(across(c(statin, ldl), ~ sum(is.na(.x))))
```

So what shall we do?

- Drop the 37 cases, or
- Something else?

# On Missing Data

## Drop the Missing = A "Complete Case" analysis

- We could drop these 37, and do a **complete case analysis** on the other 431-37 = 394 subjects. 
- We'll also create a factor (`statin_f`) with the statin information.

```{r}
dm431_cc <- dm431 %>% filter(complete.cases(ldl, statin)) %>%
  mutate(statin_f = fct_recode(factor(statin), 
                        "Statin" = "1", "No" = "0"))

mosaic::favstats(ldl ~ statin_f, data = dm431_cc) %>% 
  kable(dig = 2)
```

- HUGE assumption: The 37 missing `ldl` are MCAR.

## Missing Completely at Random (MCAR)

Our complete case analysis requires the HUGE assumption that these 37 observations are what Donald Rubin called "missing completely at random."

**Missing Completely at Random** (MCAR) means that there is no relationship between whether a data point is missing and any values in the data set, missing or observed. Thus, the missing values are just a random subset of the data.

- That is the huge assumption that is both impossible to prove and that is also tacitly made in many settings, more or less by default.
- The alternative is to consider other possible mechanisms (besides MCAR) for why data might be missing.



## Assuming data are Missing at Random (MAR)?

**Missing at Random** (MAR): the reason a data point is missing is related to some observed data, but unrelated to the actual missing values.

So we assume that we can predict the missing values effectively using other variables in the data, without causing any problems. That's a big assumption, but then we could *impute* (or fill in with predictions based on other variables) the missing data. 

So to impute predicted `ldl` values for these 37 subjects, we'd need to:

- account for the fact that we're imputing in building estimates, and
- control for the variables which (together) predict why the data were missing, and
- remember that we are making a large and unverifiable assumption about why the data are missing.

If missing data aren't MCAR or MAR, then they are MNAR.

## Three Types of Missingness

1. MCAR: Missing Completely At Random (ignorable nonresponse)
    - missing values are just a random subset of the data
    - unrealistically strong assumption in practice, although it's easy
    - makes a complete case analysis unbiased
2. MAR: Missing At Random
    - reason for missingness can be completely accounted for by variables where there is complete information
    - much more reasonable in many settings than MCAR, but impossible to verify statistically
    - imputing missing values here leads to a more robust conclusion
3. MNAR: Missing Not at Random (nonignorable nonresponse)
    - data are neither MCAR nor MAR
    - the reason the data is missing is related to its value, even after controlling for other variables.

These have different effects on the validity of the conclusions you build.

## DTDP: Example 1. (Comparing LDL by Statin Use)

Assuming MCAR, we'll press on with a complete case analysis.

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431_cc, aes(x = statin_f, y = ldl, group = statin_f)) +
  geom_violin(aes(fill = statin_f)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431_cc, aes(sample = ldl, group = statin_f)) +
  geom_qq(aes(col = statin_f)) + geom_qq_line(col = "black") +
  facet_wrap(~ statin) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "LDL Cholesterol (mg/dl)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing LDL by Statin Use in our dm431 complete cases (n = 394)")
```

## Linear Model for Example 1 (slide A)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - What is the sample mean `ldl` for those not on a statin? 
  - What is the sample mean `ldl` for statin users?
  - The point estimate for $\mu_S - \mu_N$ is ...

## Linear Model for Example 2 (slide B)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

```{r, echo = FALSE}
res1 <- tidy(app1, conf.int = T, conf.level = 0.90) %>% filter(term == "statin")
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is **`r round_half_up(res1$estimate, 2)`**
  - The 90% confidence interval for $\mu_S - \mu_N$ is ...
  
## Linear Model for Example 2 (slide C)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is `r round_half_up(res1$estimate, 2)`
  - The 90% confidence interval for $\mu_S - \mu_N$ is (**`r round_half_up(res1$conf.low, 2)`**, **`r round_half_up(res1$conf.high, 2)`**)
  
## Building Confidence Intervals for $\mu_1 - \mu_2$

The hypotheses we are testing are ($\Delta_0$ is usually zero):

- $H_0$: $\mu_1$ = $\mu_2$ + hypothesized difference $\Delta_0$ vs.
- $H_A$: $\mu_1 \neq \mu_2$  + hypothesized difference $\Delta_0$. 

Four Approaches

1. Indicator Variable Regression Model ("Pooled" t approach, or "t test" assuming equal population variances)

2. Welch t CI (t approach without assuming equal population variances)

3. Wilcoxon-Mann-Whitney Rank Sum Test (non-parametric test not assuming Normality but needing symmetry to be related to means)

4. Bootstrap confidence interval for the difference in population means (fewest assumptions of these options)

# The Pooled t procedure (same as indicator variable regression)

## Building a Pooled t CI

1. Best approach: use indicator variable regression
2. Also: direct call to t test with pooled variance estimate

```{r}
t.test(ldl ~ statin, data = dm431_cc, alt = "two.sided", mu = 0,
       var.equal = TRUE, conf.level = 0.90)
```

## `t` test can be tidied

```{r}
t1 <- tidy(t.test(ldl ~ statin, data = dm431_cc, 
                  var.equal = TRUE, conf.level = 0.90))
```

- `conf.level` must be specified to `t.test`. Otherwise, it uses 0.95.

Elements of `t1` are printed below (after rearrangement)

```{r, echo = FALSE}
t1 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t1 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

- This estimates $\mu_{NoStatin} - \mu_{Statin}$. Invert the signs of the estimate and the endpoints of the CI to estimate $\mu_{Statin} - \mu_{NoStatin}$.

## Assumptions of the Pooled T test

The standard method for comparing population means based on two independent samples is based on the t distribution, and requires the following assumptions:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances or Balanced Design] We must assume:
  - Either the population variances in the two groups are the same, so a pooled estimate of their joint variance makes sense,
  - OR the two samples are the same size (a balanced design.)

# The Welch t procedure (t approach, not assuming equal population variances)

## Assumptions of the Welch t approach

The Welch test still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed

But it doesn't require:

4.	[Equal Variances] The population variances in the two groups being compared are the same. (for instance, Welch's test still works if the larger variance $\sigma_1^2$ is more than 1.5 times as large as $\sigma_2^2$). 
  - If the design is balanced ($n_1 = n_2$) or nearly so, the impact of assuming equal variances is minimal.

Welch's t test is the default `t.test` in R.

## Building the Welch t CI

- Sensible approach when assuming Normal populations is OK, but we don't want to assume the two populations have the same variance (as pooled t requires)

```{r}
t.test(ldl ~ statin, data = dm431_cc, alt = "two.sided", mu = 0,
       conf.level = 0.90)
```

## Welch `t` test can also be tidied

```{r}
t2 <- tidy(t.test(ldl ~ statin, data = dm431_cc, 
                  conf.level = 0.90))
```

- We must specify `conf.level` in the `t.test` unless we want 0.95.

Elements of `t2` are printed below (after rearrangement)

```{r, echo = FALSE}
t2 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t2 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

- Invert signs of estimate and CI limits to get $\mu_{Statin} - \mu_{No}$.

# The Wilcoxon-Mann-Whitney Rank Sum procedure 

## Wilcoxon-Mann-Whitney Rank Sum Approach

The Wilcoxon-Mann-Whitney Rank Sum procedure requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.  [Symmetry] The two populations are each symmetrically distributed, and as a result, we're comfortable estimating the shift in location (measured by the pseudo-medians) rather than a shift in means.

But it doesn't require:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

As mentioned, it doesn't really compare population means, but instead pseudo-medians.

## Wilcoxon-Mann-Whitney Rank Sum Approach

```{r}
wilcox.test(ldl ~ statin, data = dm431_cc, 
            conf.int = TRUE, conf.level = 0.90)
```

## Rank Sum test can also be tidied

```{r}
w3 <- tidy(wilcox.test(ldl ~ statin, data = dm431_cc, 
            conf.int = TRUE, conf.level = 0.90))
```

- Specify `conf.int` and `conf.level` in the `wilcox.test`.

Elements of `w3` are printed below (after rearrangement)

```{r, echo = FALSE}
w3 %>% 
  select(method, alternative, statistic) %>% 
  kable(digits = 2)

w3 %>% 
  select(estimate, conf.low, conf.high, p.value) %>% 
  kable(digits = 2)
```

- Invert signs of estimate and CI to describe shift from No to Statin.


## The Bootstrap

This bootstrap approach to comparing population means using two independent samples still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.

but does not require either of the other two assumptions:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

The bootstrap procedure I use in R was adapted from Frank Harrell and colleagues. http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/BootstrapMeansSoftware

## The `bootdif` function

The procedure requires the definition of a function, which I have adapted a bit, called `bootdif`, which is part of the `Love-boost.R` script we loaded earlier.

As in our previous bootstrap procedures, we are sampling (with replacement) a series of many data sets (default: 2000).

- Here, we are building bootstrap samples based on the LDL levels in the two independent samples (statin users vs. non-users.) 
- For each bootstrap sample, we are calculating a mean difference between the two groups (statin vs. no statin.)
- We then determine the 2.5th and 97.5th percentile of the resulting distribution of mean differences (for a 95% confidence interval).  

## Using `bootdif` to compare mean(LDL) by statin

So, to compare LDL (our outcome) across the two levels of statin (our grouping factor) for the adult patients with diabetes in NE Ohio, run...

```{r}
set.seed(20201008)
boot4 <- dm431_cc %$% bootdif(ldl, statin, conf.level = 0.90)
boot4
```

- The two columns must be separated here with a comma rather than a tilde (`~`), and are specified using `$` notation. 
- This CI estimates $\mu_{Statin} - \mu_{NoStatin}$. Observe the listed sample mean difference for the necessary context. 
- If we change the `set.seed`, we'll get different endpoints for our CI.
- Note that we can infer the *p* value is above 0.10 from the CI. Why?


## Results for the LDL and Statin Study

Procedure     | *p* for $H_0: \mu_S = \mu_N$ | 90% CI for $\mu_S - \mu_N$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 2)` | (`r -1*round_half_up(t1$conf.high, 1)`, `r -1*round_half_up(t1$conf.low, 1)`)
Welch t   | `r round_half_up(t2$p.value, 2)` | (`r -1*round_half_up(t2$conf.high, 1)`, `r -1*round_half_up(t2$conf.low, 1)`)
Bootstrap   | *p* > 0.100 | (`r round_half_up(boot4[2],1)`, `r round_half_up(boot4[3],1)`)

Procedure     | *p* for $H_0: psmed_S = psmed_N$ | 90% CI for S - N shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 2)` | (`r -1*round_half_up(w3$conf.high, 2)`, `r -1*round_half_up(w3$conf.low, 2)`)

**Which method should we use?**

## Which Method Should We Use?

1. Plot the distributions of the two independent samples.
2. Does it seem reasonable to assume that **each** distribution (here, both `ldl` in statin users and `ldl` in non-users) follows an approximately Normal distribution?

- If Yes, Normal models seem fairly appropriate, then
  - use the indicator variable regression (pooled t test) if the sample sizes are nearly the same, or if the sample variances are reasonably similar
  - use the Welch's t test, otherwise (default `t.test` in R)
- If No, Normal models don't seem appropriate at all, then
  - compare means using the bootstrap via `bootdif`, or
  - compare pseudo-medians using the WMW rank sum test

What did we see in our `ldl` data? 

## LDL, within groups defined by `statin`

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431_cc, aes(x = statin_f, y = ldl, group = statin_f)) +
  geom_violin(aes(fill = statin_f)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431_cc, aes(sample = ldl, group = statin_f)) +
  geom_qq(aes(col = statin_f)) + geom_qq_line(col = "black") +
  facet_wrap(~ statin) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "LDL Cholesterol (mg/dl)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing LDL by Statin Use in our dm431 complete cases (n = 394)")
```

## Results for the LDL and Statin Study

Procedure     | *p* for $H_0: \mu_S = \mu_N$ | 90% CI for $\mu_S - \mu_N$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 2)` | (`r -1*round_half_up(t1$conf.high, 1)`, `r -1*round_half_up(t1$conf.low, 1)`)
Welch t   | `r round_half_up(t2$p.value, 2)` | (`r -1*round_half_up(t2$conf.high, 1)`, `r -1*round_half_up(t2$conf.low, 1)`)
Bootstrap   | *p* > 0.100 | (`r round_half_up(boot4[2],1)`, `r round_half_up(boot4[3],1)`)

Procedure     | *p* for $H_0: psmed_S = psmed_N$ | 90% CI for S - N shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 2)` | (`r -1*round_half_up(w3$conf.high, 2)`, `r -1*round_half_up(w3$conf.low, 2)`)

What conclusions should we draw, at $\alpha$ = 0.10?

# Example 2 (Comparing SBP by Sex) slides follow, for you to review on your own. The main difference is that we have no missing values in SBP or Sex in the `dm431` data.

## `dm431` Example 2. (Comparing SBP by Sex)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r, message = FALSE}
mosaic::favstats(sbp ~ sex, data = dm431) %>% kable(dig = 2)
```

- What is the outcome here? 
- What are the two exposure groups we are comparing?
- What are the sample means, $\bar{x}_F$ and $\bar{x}_M$?
- Point estimate of the difference in population means, $\mu_F - \mu_M$?

## DTDP for Example 2. (Comparing SBP by Sex)

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431, aes(x = sex, y = sbp, group = sex)) +
  geom_violin(aes(fill = sex)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431, aes(sample = sbp, group = sex)) +
  geom_qq(aes(col = sex)) + geom_qq_line(col = "black") +
  facet_wrap(~ sex) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "Systolic Blood Pressure (mm Hg)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 2. Comparing SBP by sex in our dm431 data")
```

## Linear Model for Example 2 (slide A)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r}
m1 <- lm(sbp ~ sex, data = dm431)

tidy(m1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - What is the sample mean `sbp` for females? 
  - What is the sample mean `sbp` for males?
  - The point estimate for $\mu_F - \mu_M$ is ...

## Linear Model for Example 2 (slide B)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r}
m1 <- lm(sbp ~ sex, data = dm431)

tidy(m1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

```{r, echo = FALSE}
res <- tidy(m1, conf.int = T, conf.level = 0.90) %>% filter(term == "sexM")
```

- What can we learn from this output? 
  - The point estimate for $\mu_F - \mu_M$ is `r -1*round_half_up(res$estimate, 2)`
  - The 90% confidence interval for $\mu_F - \mu_M$ is (**`r -1*round_half_up(res$conf.high, 2)`**, **`r -1*round_half_up(res$conf.low, 2)`**)
  
## Building a Pooled t CI: Example 2

1. Best approach: use indicator variable regression
2. Also: direct call to t test with pooled variance estimate

```{r}
t.test(sbp ~ sex, data = dm431, alt = "two.sided", mu = 0,
       var.equal = TRUE, conf.level = 0.90)
```

## `t` test can be tidied

```{r}
t1 <- tidy(t.test(sbp ~ sex, data = dm431, 
                  var.equal = TRUE, conf.level = 0.90))
```

- `conf.level` must be specified to `t.test`. Otherwise, it uses 0.95.

Elements of `t1` are printed below (after rearrangement)

```{r, echo = FALSE}
t1 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t1 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

## Building the Welch t CI: Example 2

- Sensible approach when assuming Normal populations is OK, but we don't want to assume the two populations have the same variance (as pooled t requires)

```{r}
t.test(sbp ~ sex, data = dm431, alt = "two.sided", mu = 0,
       conf.level = 0.90)
```

## Welch `t` test can also be tidied

```{r}
t2 <- tidy(t.test(sbp ~ sex, data = dm431, 
                  conf.level = 0.90))
```

- We must specify `conf.level` in the `t.test` unless we want 0.95.

Elements of `t2` are printed below (after rearrangement)

```{r, echo = FALSE}
t2 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t2 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

## Wilcoxon-Mann-Whitney Rank Sum: Example 2

```{r}
wilcox.test(sbp ~ sex, data = dm431, 
            conf.int = TRUE, conf.level = 0.90)
```

## Rank Sum test can also be tidied

```{r}
w3 <- tidy(wilcox.test(sbp ~ sex, data = dm431, 
            conf.int = TRUE, conf.level = 0.90))
```

- Specify `conf.int` and `conf.level` in the `wilcox.test`.

Elements of `w3` are printed below (after rearrangement)

```{r, echo = FALSE}
w3 %>% 
  select(method, alternative, statistic) %>% 
  kable(digits = 2)

w3 %>% 
  select(estimate, conf.low, conf.high, p.value) %>% 
  kable(digits = 2)
```

## Using `bootdif` to compare mean(SBP) by Sex

So, to compare systolic BP (our outcome) across the two levels of sex (our grouping factor) for the adult patients with diabetes in NE Ohio, run...

```{r}
set.seed(431431)
boot4 <- dm431 %$% bootdif(sbp, sex, conf.level = 0.90)
boot4
```

- This CI estimates $\mu_{M} - \mu_{F}$: observe the listed sample mean difference for the necessary context. 
- Invert the signs to estimate $\mu_{F} - \mu_{M}$.
- Again the *p* value must be larger than 0.10 since 0 is in the 90% CI.

## Results for the SBP and Sex Study

Procedure     | *p* for $H_0: \mu_F = \mu_M$ | 90% CI for $\mu_F - \mu_M$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 4)` | (`r round_half_up(t1$conf.low, 2)`, `r round_half_up(t1$conf.high, 2)`)
Welch t   | `r round_half_up(t2$p.value, 4)` | (`r round_half_up(t2$conf.low, 2)`, `r round_half_up(t2$conf.high, 2)`)
Bootstrap   | *p* > 0.100 | (`r -1*round_half_up(boot4[3],2)`, `r -1*round_half_up(boot4[2],2)`)

Procedure     | *p* for $H_0: psmed_F = psmed_M$ | 90% CI for F - M shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 4)` | (`r round_half_up(w3$conf.low, 2)`, `r round_half_up(w3$conf.high, 2)`)

**Which method should we use?**

## Systolic BP, within groups defined by sex

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431, aes(x = sex, y = sbp, group = sex)) +
  geom_violin(aes(fill = sex)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431, aes(sample = sbp, group = sex)) +
  geom_qq(aes(col = sex)) + geom_qq_line(col = "black") +
  facet_wrap(~ sex) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "Systolic Blood Pressure (mm Hg)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing SBP by sex in our dm431 data")
```

## Results for the SBP and Sex Study

Procedure     | *p* for $H_0: \mu_F = \mu_M$ | 90% CI for $\mu_F - \mu_M$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 4)` | (`r round_half_up(t1$conf.low, 2)`, `r round_half_up(t1$conf.high, 2)`)
Welch t   | `r round_half_up(t2$p.value, 4)` | (`r round_half_up(t2$conf.low, 2)`, `r round_half_up(t2$conf.high, 2)`)
Bootstrap   | *p* > 0.100 | (`r -1*round_half_up(boot4[3],2)`, `r -1*round_half_up(boot4[2],2)`)

Procedure     | *p* for $H_0: psmed_F = psmed_M$ | 90% CI for F - M shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 4)` | (`r round_half_up(w3$conf.low, 2)`, `r round_half_up(w3$conf.high, 2)`)


What conclusions should we draw, at $\alpha$ = 0.10?

