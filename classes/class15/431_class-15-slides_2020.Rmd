---
title: "431 Class 15"
author: "thomaselove.github.io/431"
date: "2020-10-13"
output:
  beamer_presentation:
    theme: "Madrid"
    fonttheme: "structurebold"
    colortheme: "whale"
    fig_height: 5.5
    fig_caption: false
---

```{r set-options, echo=FALSE, cache=FALSE, message = FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 55)
```

## Today's Agenda

- Comparing Means using Regression Models
  - Comparing Two Groups
  - Comparing More Than Two Groups
  - What you'll be using in Project A
- Alternatives for Comparing Means using Two Independent Samples
  - Welch's t test (not assuming equal population variances)
  - Bootstrap methods for comparing means in 2 samples
  - Rank-based alternatives (Wilcoxon-Mann-Whitney)

## Today's R Packages and Data

```{r, message = FALSE}
library(broom)
library(ggrepel)
library(janitor)
library(knitr)
library(magrittr)
library(patchwork)
library(tidyverse)

theme_set(theme_bw())

source("data/Love-boost.R") # new today!

dm431 <- readRDS("data/dm431_2020.Rds")
```

## Comparing Means with `dm431`

Our population: ALL adults ages 31-70 seen for care this year and two years ago who live in Northeast Ohio with a diabetes diagnosis.

Our `dm431` sample: 431 of those people, drawn in a way we hope is representative (but certainly isn't random).

1. Can we estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not?

2. Can we estimate the difference between people with four types of `insurance` in terms of their population mean hemoglobin A1c? (or maybe their diastolic BP?)

3. Can we estimate the difference between females and males in terms of the population mean systolic blood pressure? 

# Comparing Population Means using Regression Models

## `dm431` Example 1 (Comparing LDL by Statin usage)

Estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not.

```{r, message = FALSE}
mosaic::favstats(ldl ~ statin, data = dm431) %>% 
  kable(digits = 2)
```

>- What is the outcome here? 
>- What are the two exposure groups we are comparing?
>- What are the sample means, $\bar{x}_{Statin}$ and $\bar{x}_{No Statin}$?
>- How might we estimate the difference in population means, $\mu_S - \mu_N$?
>- Is there a problem in these data we need to deal with?

## How much missing data do we have?

Do we have missing values in both columns, or just one?

```{r}
dm431 %>% summarize(across(c(statin, ldl), ~ sum(is.na(.x))))
```

So what shall we do?

- Drop the 37 cases, or
- Something else?

# On Missing Data

## Drop the Missing = A "Complete Case" analysis

- We could drop these 37, and do a **complete case analysis** on the other 431-37 = 394 subjects. 
- We'll also create a factor (`statin_f`) with the statin information.

```{r}
dm431_cc <- dm431 %>% filter(complete.cases(ldl, statin)) %>%
  mutate(statin_f = fct_recode(factor(statin), 
                        "Statin" = "1", "No" = "0"))

mosaic::favstats(ldl ~ statin_f, data = dm431_cc) %>% 
  kable(dig = 2)
```

- HUGE assumption: The 37 missing `ldl` are MCAR.

## Missing Completely at Random (MCAR)

Our complete case analysis requires the HUGE assumption that these 37 observations are what Donald Rubin called "missing completely at random."

**Missing Completely at Random** (MCAR) means that there is no relationship between whether a data point is missing and any values in the data set, missing or observed. Thus, the missing values are just a random subset of the data.

- That is the huge assumption that is both impossible to prove and that is also tacitly made in many settings, more or less by default.
- The alternative is to consider other possible mechanisms (besides MCAR) for why data might be missing.



## Assuming data are Missing at Random (MAR)?

**Missing at Random** (MAR): the reason a data point is missing is related to some observed data, but unrelated to the actual missing values.

So we assume that we can predict the missing values effectively using other variables in the data, without causing any problems. That's a big assumption, but then we could *impute* (or fill in with predictions based on other variables) the missing data. 

So to impute predicted `ldl` values for these 37 subjects, we'd need to:

- account for the fact that we're imputing in building estimates, and
- control for the variables which (together) predict why the data were missing, and
- remember that we are making a large and unverifiable assumption about why the data are missing.

If missing data aren't MCAR or MAR, then they are MNAR.

## Three Types of Missingness

1. MCAR: Missing Completely At Random (ignorable nonresponse)
    - missing values are just a random subset of the data
    - unrealistically strong assumption in practice, although it's easy
    - makes a complete case analysis unbiased
2. MAR: Missing At Random
    - reason for missingness can be completely accounted for by variables where there is complete information
    - much more reasonable in many settings than MCAR, but impossible to verify statistically
    - imputing missing values here leads to a more robust conclusion
3. MNAR: Missing Not at Random (nonignorable nonresponse)
    - data are neither MCAR nor MAR
    - the reason the data is missing is related to its value, even after controlling for other variables.

These have different effects on the validity of the conclusions you build.

## DTDP: Example 1. (Comparing LDL by Statin Use)

Assuming MCAR, we'll press on with a complete case analysis.

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431_cc, aes(x = statin_f, y = ldl, group = statin_f)) +
  geom_violin(aes(fill = statin_f)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431_cc, aes(sample = ldl, group = statin_f)) +
  geom_qq(aes(col = statin_f)) + geom_qq_line(col = "black") +
  facet_wrap(~ statin) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "LDL Cholesterol (mg/dl)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing LDL by Statin Use in our dm431 complete cases (n = 394)")
```

## Linear Model for Example 1 (slide A)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - What is the sample mean `ldl` for those not on a statin? 
  - What is the sample mean `ldl` for statin users?
  - The point estimate for $\mu_S - \mu_N$ is ...

## Linear Model for Example 1 (slide B)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

```{r, echo = FALSE}
res1 <- tidy(app1, conf.int = T, conf.level = 0.90) %>% filter(term == "statin")
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is **`r round_half_up(res1$estimate, 2)`**
  - The 90% confidence interval for $\mu_S - \mu_N$ is ...
  
## Linear Model for Example 1 (slide C)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is `r round_half_up(res1$estimate, 2)`
  - The 90% confidence interval for $\mu_S - \mu_N$ is (**`r round_half_up(res1$conf.low, 2)`**, **`r round_half_up(res1$conf.high, 2)`**)

## Augment our model to get fitted/residual values?

```{r}
aug1 <- augment(app1, dm431_cc)

aug1 %>% select(subject, statin, ldl, .fitted, .resid) %>%
  slice(1, 6, 206, 394)
```

Here, I'm just using `slice` to pick out four values from the distribution (two with statin, two without and two with a positive and two with a negative residual.)

## Residual Plots for Example 1 `app1`?

```{r, echo = FALSE}
p1 <- ggplot(aug1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = aug1 %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "app1 Residuals vs. Fitted",
       x = "Fitted LDL from app1",
       y = "Residuals from app1")

p2 <- ggplot(aug1, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "app1 Residuals",
       y = "")

p3 <- ggplot(aug1, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

## Conclusions So Far: Example 1

- The point estimate for $\mu_S - \mu_N$ is `r round_half_up(res1$estimate, 2)`
- The 90% confidence interval for $\mu_S - \mu_N$ is (**`r round_half_up(res1$conf.low, 2)`**, **`r round_half_up(res1$conf.high, 2)`**)
- There is some evidence of non-Normality in the residuals after this regression model. 
  - Perhaps the assumption that the difference $\mu_S - \mu_N$ is Normally distributed is in question. This will eventually lead to alternatives to the t test, discussed later in these slides.
  
But for now, let's look at an example where we compare means across more than just two groups.

# Example 2 (Comparing Hemoglobin A1c by Insurance type)

## Comparing A1c by Insurance Type in `dm431`

```{r}
dm431 %>% select(insurance, a1c) %>% glimpse()
```

```{r}
dm431 %$% mosaic::favstats(a1c ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 2)
```

## Distribution of A1c in insurance groups 

```{r, echo = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = a1c)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = a1c, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Hemoglobin A1c (in %)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## Code for previous slide

```{r, eval = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = a1c)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = a1c, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Hemoglobin A1c (in %)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## We'll assume MCAR and run a model

```{r}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

modA <- lm(a1c ~ insurance, data = dm_comp)
modA
```

- It was very helpful that `insurance` was a factor already.

## Model A Fit Summary

```{r}
glance(modA) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `a1c` in these data?

## Model A Coefficients

```{r}
tidy(modA, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 2)
```

- Which insurance type is associated with the highest (worst) A1c? 
- Which has the lowest predicted A1c? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augA <- augment(modA, dm_comp)

augA %>% select(subject, insurance, a1c, .fitted, .resid) %>% 
  head() %>% kable(dig = 2)
```

## Residual Plots for `modA`

```{r, echo = FALSE}
p1 <- ggplot(augA, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augA %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modA Residuals vs. Fitted",
       x = "Fitted A1c from modA",
       y = "Residuals from modA")

p2 <- ggplot(augA, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modA Residuals",
       y = "")

p3 <- ggplot(augA, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

## Try `log(a1c)` as our outcome instead?

```{r, echo = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = log(a1c))) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = log(a1c), col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "log of Hemoglobin A1c") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## log(A1c) by Insurance Type in `dm431`

```{r}
dm431 %$% mosaic::favstats(log(a1c) ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 3)
```

## We'll assume MCAR and run the logged A1c model

```{r}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

modB <- lm(log(a1c) ~ insurance, data = dm_comp)
modB
```

## Model B Fit Summary

```{r}
glance(modB) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `log(a1c)` in these data?

## Model B Coefficients

```{r}
tidy(modB, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 3)
```

- Which insurance type is associated with the highest (worst) A1c? 
- Which has the lowest predicted A1c? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augB <- augment(modB, dm_comp) %>% 
  mutate(log_a1c = log(a1c))

augB %>% select(subject, insurance, a1c, 
                log_a1c, .fitted, .resid) %>% 
  head() %>% kable(dig = 3)
```

## Residual Plots for `modB` 

```{r, echo = FALSE}
p1 <- ggplot(augB, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augB %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modB Residuals vs. Fitted",
       x = "Fitted log(A1c) from modB",
       y = "Residuals from modB")

p2 <- ggplot(augB, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modB Residuals",
       y = "")

p3 <- ggplot(augB, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

# Try `dbp` as our outcome instead?

## Diastolic BP by Insurance Type in `dm431`

```{r}
dm431 %$% mosaic::favstats(dbp ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 1)
```

## Compare `dbp` across insurance types?

```{r, echo = FALSE}
p1 <- ggplot(dm_comp, aes(x = insurance, y = dbp)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = dbp, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Diastolic BP (mm Hg)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## We'll assume MCAR and try to predict `dbp`

```{r}
modD <- lm(dbp ~ insurance, data = dm431)
modD
```

## Model D Fit Summary

```{r}
glance(modD) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `dbp` in these data?

## Model D Coefficients

```{r}
tidy(modD, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 1)
```

- Which insurance type is associated with the highest (worst) `dbp`? 
- Which has the lowest predicted `dbp`? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augD <- augment(modD, dm431) 

augD %>% select(subject, insurance, dbp, .fitted, .resid) %>% 
  head() %>% kable(dig = 2)
```

## Residual Plots for `modD`

```{r, echo = FALSE}
p1 <- ggplot(augD, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augD %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modD Residuals vs. Fitted",
       x = "Fitted dbp from modD",
       y = "Residuals from modD")

p2 <- ggplot(augB, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modD Residuals",
       y = "")

p3 <- ggplot(augB, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

# That's the end of the material I expect you to use in Project A. All remaining slides were moved to Class 16.

