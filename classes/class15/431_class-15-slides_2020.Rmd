---
title: "431 Class 15"
author: "thomaselove.github.io/431"
date: "2020-10-13"
output:
  beamer_presentation:
    theme: "Madrid"
    fonttheme: "structurebold"
    colortheme: "whale"
    fig_height: 5.5
    fig_caption: false
---

```{r set-options, echo=FALSE, cache=FALSE, message = FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 55)
```

## Today's Agenda

- Comparing Means using Regression Models
  - Comparing Two Groups
  - Comparing More Than Two Groups
  - What you'll be using in Project A
- Alternatives for Comparing Means using Two Independent Samples
  - Welch's t test (not assuming equal population variances)
  - Bootstrap methods for comparing means in 2 samples
  - Rank-based alternatives (Wilcoxon-Mann-Whitney)

## Today's R Packages and Data

```{r, message = FALSE}
library(broom)
library(ggrepel)
library(janitor)
library(knitr)
library(magrittr)
library(patchwork)
library(tidyverse)

theme_set(theme_bw())

source("data/Love-boost.R") # new today!

dm431 <- readRDS("data/dm431_2020.Rds")
```

## Comparing Means with `dm431`

Our population: ALL adults ages 31-70 seen for care this year and two years ago who live in Northeast Ohio with a diabetes diagnosis.

Our `dm431` sample: 431 of those people, drawn in a way we hope is representative (but certainly isn't random).

1. Can we estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not?

2. Can we estimate the difference between people with four types of `insurance` in terms of their population mean hemoglobin A1c? (or maybe their diastolic BP?)

3. Can we estimate the difference between females and males in terms of the population mean systolic blood pressure? 

# Comparing Population Means using Regression Models

## `dm431` Example 1 (Comparing LDL by Statin usage)

Estimate the difference in the population mean LDL cholesterol for those who have a statin prescription as compared to those who do not.

```{r, message = FALSE}
mosaic::favstats(ldl ~ statin, data = dm431) %>% 
  kable(digits = 2)
```

>- What is the outcome here? 
>- What are the two exposure groups we are comparing?
>- What are the sample means, $\bar{x}_{Statin}$ and $\bar{x}_{No Statin}$?
>- How might we estimate the difference in population means, $\mu_S - \mu_N$?
>- Is there a problem in these data we need to deal with?

## How much missing data do we have?

Do we have missing values in both columns, or just one?

```{r}
dm431 %>% summarize(across(c(statin, ldl), ~ sum(is.na(.x))))
```

So what shall we do?

- Drop the 37 cases, or
- Something else?

# On Missing Data

## Drop the Missing = A "Complete Case" analysis

- We could drop these 37, and do a **complete case analysis** on the other 431-37 = 394 subjects. 
- We'll also create a factor (`statin_f`) with the statin information.

```{r}
dm431_cc <- dm431 %>% filter(complete.cases(ldl, statin)) %>%
  mutate(statin_f = fct_recode(factor(statin), 
                        "Statin" = "1", "No" = "0"))

mosaic::favstats(ldl ~ statin_f, data = dm431_cc) %>% 
  kable(dig = 2)
```

- HUGE assumption: The 37 missing `ldl` are MCAR.

## Missing Completely at Random (MCAR)

Our complete case analysis requires the HUGE assumption that these 37 observations are what Donald Rubin called "missing completely at random."

**Missing Completely at Random** (MCAR) means that there is no relationship between whether a data point is missing and any values in the data set, missing or observed. Thus, the missing values are just a random subset of the data.

- That is the huge assumption that is both impossible to prove and that is also tacitly made in many settings, more or less by default.
- The alternative is to consider other possible mechanisms (besides MCAR) for why data might be missing.



## Assuming data are Missing at Random (MAR)?

**Missing at Random** (MAR): the reason a data point is missing is related to some observed data, but unrelated to the actual missing values.

So we assume that we can predict the missing values effectively using other variables in the data, without causing any problems. That's a big assumption, but then we could *impute* (or fill in with predictions based on other variables) the missing data. 

So to impute predicted `ldl` values for these 37 subjects, we'd need to:

- account for the fact that we're imputing in building estimates, and
- control for the variables which (together) predict why the data were missing, and
- remember that we are making a large and unverifiable assumption about why the data are missing.

If missing data aren't MCAR or MAR, then they are MNAR.

## Three Types of Missingness

1. MCAR: Missing Completely At Random (ignorable nonresponse)
    - missing values are just a random subset of the data
    - unrealistically strong assumption in practice, although it's easy
    - makes a complete case analysis unbiased
2. MAR: Missing At Random
    - reason for missingness can be completely accounted for by variables where there is complete information
    - much more reasonable in many settings than MCAR, but impossible to verify statistically
    - imputing missing values here leads to a more robust conclusion
3. MNAR: Missing Not at Random (nonignorable nonresponse)
    - data are neither MCAR nor MAR
    - the reason the data is missing is related to its value, even after controlling for other variables.

These have different effects on the validity of the conclusions you build.

## DTDP: Example 1. (Comparing LDL by Statin Use)

Assuming MCAR, we'll press on with a complete case analysis.

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431_cc, aes(x = statin_f, y = ldl, group = statin_f)) +
  geom_violin(aes(fill = statin_f)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431_cc, aes(sample = ldl, group = statin_f)) +
  geom_qq(aes(col = statin_f)) + geom_qq_line(col = "black") +
  facet_wrap(~ statin) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "LDL Cholesterol (mg/dl)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing LDL by Statin Use in our dm431 complete cases (n = 394)")
```

## Linear Model for Example 1 (slide A)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - What is the sample mean `ldl` for those not on a statin? 
  - What is the sample mean `ldl` for statin users?
  - The point estimate for $\mu_S - \mu_N$ is ...

## Linear Model for Example 1 (slide B)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

```{r, echo = FALSE}
res1 <- tidy(app1, conf.int = T, conf.level = 0.90) %>% filter(term == "statin")
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is **`r round_half_up(res1$estimate, 2)`**
  - The 90% confidence interval for $\mu_S - \mu_N$ is ...
  
## Linear Model for Example 1 (slide C)

Estimate the difference in population mean LDL cholesterol among people taking a statin as compared to those not taking a statin.

```{r}
app1 <- lm(ldl ~ statin, data = dm431_cc)

tidy(app1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - The point estimate for $\mu_S - \mu_N$ is `r round_half_up(res1$estimate, 2)`
  - The 90% confidence interval for $\mu_S - \mu_N$ is (**`r round_half_up(res1$conf.low, 2)`**, **`r round_half_up(res1$conf.high, 2)`**)

## Augment our model to get fitted/residual values?

```{r}
aug1 <- augment(app1, dm431_cc)

aug1 %>% select(subject, statin, ldl, .fitted, .resid) %>%
  head()
```

## Residual Plots for Example 1 `app1`?

```{r, echo = FALSE}
p1 <- ggplot(aug1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = aug1 %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "app1 Residuals vs. Fitted",
       x = "Fitted LDL from app1",
       y = "Residuals from app1")

p2 <- ggplot(aug1, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "app1 Residuals",
       y = "")

p3 <- ggplot(aug1, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

## Conclusions So Far: Example 1

- The point estimate for $\mu_S - \mu_N$ is `r round_half_up(res1$estimate, 2)`
- The 90% confidence interval for $\mu_S - \mu_N$ is (**`r round_half_up(res1$conf.low, 2)`**, **`r round_half_up(res1$conf.high, 2)`**)
- There is some evidence of non-Normality in the residuals after this regression model. 
  - Perhaps the assumption that the difference $mu_S - \mu_N$ is Normally distributed is in question. This will eventually lead to alternatives to the t test, discussed later in these slides.
  
But for now, let's look at an example where we compare means across more than just two groups.

# Example 2 (Comparing Hemoglobin A1c by Insurance type)

## Comparing A1c by Insurance Type in `dm431`

```{r}
dm431 %>% select(insurance, a1c) %>% glimpse()
```

```{r}
dm431 %$% mosaic::favstats(a1c ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 2)
```

## Distribution of A1c in insurance groups 

```{r, echo = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = a1c)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = a1c, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Hemoglobin A1c (in %)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## Code for previous slide

```{r, eval = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = a1c)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = a1c, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Hemoglobin A1c (in %)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## We'll assume MCAR and run a model

```{r}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

modA <- lm(a1c ~ insurance, data = dm_comp)
modA
```

- It was very helpful that `insurance` was a factor already.

## Model A Fit Summary

```{r}
glance(modA) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `a1c` in these data?

## Model A Coefficients

```{r}
tidy(modA, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 2)
```

- Which insurance type is associated with the highest (worst) A1c? 
- Which has the lowest predicted A1c? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augA <- augment(modA, dm_comp)

augA %>% select(subject, insurance, a1c, .fitted, .resid) %>% 
  head() %>% kable(dig = 2)
```

## Residual Plots for `modA`

```{r, echo = FALSE}
p1 <- ggplot(augA, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augA %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modA Residuals vs. Fitted",
       x = "Fitted A1c from modA",
       y = "Residuals from modA")

p2 <- ggplot(augA, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modA Residuals",
       y = "")

p3 <- ggplot(augA, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

## Try `log(a1c)` as our outcome instead?

```{r, echo = FALSE}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

p1 <- ggplot(dm_comp, aes(x = insurance, y = log(a1c))) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = log(a1c), col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "log of Hemoglobin A1c") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## log(A1c) by Insurance Type in `dm431`

```{r}
dm431 %$% mosaic::favstats(log(a1c) ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 3)
```

## We'll assume MCAR and run the logged A1c model

```{r}
dm_comp <- dm431 %>% 
  filter(complete.cases(a1c, insurance))

modB <- lm(log(a1c) ~ insurance, data = dm_comp)
modB
```

## Model B Fit Summary

```{r}
glance(modB) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `log(a1c)` in these data?

## Model B Coefficients

```{r}
tidy(modB, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 3)
```

- Which insurance type is associated with the highest (worst) A1c? 
- Which has the lowest predicted A1c? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augB <- augment(modB, dm_comp) %>% 
  mutate(log_a1c = log(a1c))

augB %>% select(subject, insurance, a1c, 
                log_a1c, .fitted, .resid) %>% 
  head() %>% kable(dig = 3)
```

## Residual Plots for `modB` 

```{r, echo = FALSE}
p1 <- ggplot(augB, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augB %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modB Residuals vs. Fitted",
       x = "Fitted log(A1c) from modB",
       y = "Residuals from modB")

p2 <- ggplot(augB, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modB Residuals",
       y = "")

p3 <- ggplot(augB, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

# Try `dbp` as our outcome instead?

## Diastolic BP by Insurance Type in `dm431`

```{r}
dm431 %$% mosaic::favstats(dbp ~ insurance) %>% 
  rename(na = missing) %>% kable(dig = 1)
```

## Compare `dbp` across insurance types?

```{r, echo = FALSE}
p1 <- ggplot(dm_comp, aes(x = insurance, y = dbp)) +
  geom_violin() +
  geom_boxplot(aes(fill = insurance), width = 0.2) + 
  guides(fill = FALSE)

p2 <- ggplot(dm_comp, aes(sample = dbp, col = insurance)) +
  geom_qq() + geom_qq_line(col = "black") + 
  guides(col = FALSE) +
  theme(aspect.ratio = 1) +
  labs(y = "Diastolic BP (mm Hg)") +
  facet_wrap(~ insurance)

p1 + p2 + plot_layout(widths = c(2,3))
```

## We'll assume MCAR and try to predict `dbp`

```{r}
modD <- lm(dbp ~ insurance, data = dm431)
modD
```

## Model D Fit Summary

```{r}
glance(modD) %>% 
  select(r.squared, statistic, df, df.residual, 
         p.value, sigma, nobs) %>% 
  kable(dig = c(3, 2, 0, 0, 4, 2, 0))
```

What can we conclude about whether `insurance` is an effective predictor of `dbp` in these data?

## Model D Coefficients

```{r}
tidy(modD, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high) %>%
  kable(dig = 1)
```

- Which insurance type is associated with the highest (worst) `dbp`? 
- Which has the lowest predicted `dbp`? Are these results surprising? 

## Making Predictions with `augment`

```{r}
augD <- augment(modD, dm431) 

augD %>% select(subject, insurance, dbp, .fitted, .resid) %>% 
  head() %>% kable(dig = 2)
```

## Residual Plots for `modD`

```{r, echo = FALSE}
p1 <- ggplot(augD, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F,
              lty = "dashed", col = "black") +
  geom_text_repel(data = augD %>% 
                    slice_max(abs(.resid), n = 3), 
                  aes(label = subject)) +
  labs(title = "modD Residuals vs. Fitted",
       x = "Fitted dbp from modD",
       y = "Residuals from modD")

p2 <- ggplot(augB, aes(sample = .resid)) +
  geom_qq() + geom_qq_line(col = "red") + 
  labs(title = "modD Residuals",
       y = "")

p3 <- ggplot(augB, aes(y = .resid, x = "")) +
  geom_violin(fill = "goldenrod") +
  geom_boxplot(width = 0.5) + 
  labs(y = "", x = "")

p1 + p2 + p3 + plot_layout(widths = c(5, 4, 1))
```

# That's the end of the material I expect you to use in Project A. Now, let's go back to Example 1. (Comparing LDL by Statin use)

## Building Confidence Intervals for $\mu_1 - \mu_2$

The hypotheses we are testing are ($\Delta_0$ is usually zero):

- $H_0$: $\mu_1$ = $\mu_2$ + hypothesized difference $\Delta_0$ vs.
- $H_A$: $\mu_1 \neq \mu_2$  + hypothesized difference $\Delta_0$. 

Four Approaches

1. Indicator Variable Regression Model ("Pooled" t approach, or "t test" assuming equal population variances)

2. Welch t CI (t approach without assuming equal population variances)

3. Wilcoxon-Mann-Whitney Rank Sum Test (non-parametric test not assuming Normality but needing symmetry to be related to means)

4. Bootstrap confidence interval for the difference in population means (fewest assumptions of these options)

# The Pooled t procedure (same as indicator variable regression)

## Building a Pooled t CI

1. Best approach: use indicator variable regression
2. Also: direct call to t test with pooled variance estimate

```{r}
t.test(ldl ~ statin, data = dm431_cc, alt = "two.sided", mu = 0,
       var.equal = TRUE, conf.level = 0.90)
```

## `t` test can be tidied

```{r}
t1 <- tidy(t.test(ldl ~ statin, data = dm431_cc, 
                  var.equal = TRUE, conf.level = 0.90))
```

- `conf.level` must be specified to `t.test`. Otherwise, it uses 0.95.

Elements of `t1` are printed below (after rearrangement)

```{r, echo = FALSE}
t1 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t1 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

- This estimates $\mu_{NoStatin} - \mu_{Statin}$. Invert the signs of the estimate and the endpoints of the CI to estimate $\mu_{Statin} - \mu_{NoStatin}$.

## Assumptions of the Pooled T test

The standard method for comparing population means based on two independent samples is based on the t distribution, and requires the following assumptions:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances or Balanced Design] We must assume:
  - Either the population variances in the two groups are the same, so a pooled estimate of their joint variance makes sense,
  - OR the two samples are the same size (a balanced design.)

# The Welch t procedure (t approach, not assuming equal population variances)

## Assumptions of the Welch t approach

The Welch test still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.	[Normal Population] The two populations are each Normally distributed

But it doesn't require:

4.	[Equal Variances] The population variances in the two groups being compared are the same. (for instance, Welch's test still works if the larger variance $\sigma_1^2$ is more than 1.5 times as large as $\sigma_2^2$). 
  - If the design is balanced ($n_1 = n_2$) or nearly so, the impact of assuming equal variances is minimal.

Welch's t test is the default `t.test` in R.

## Building the Welch t CI

- Sensible approach when assuming Normal populations is OK, but we don't want to assume the two populations have the same variance (as pooled t requires)

```{r}
t.test(ldl ~ statin, data = dm431_cc, alt = "two.sided", mu = 0,
       conf.level = 0.90)
```

## Welch `t` test can also be tidied

```{r}
t2 <- tidy(t.test(ldl ~ statin, data = dm431_cc, 
                  conf.level = 0.90))
```

- We must specify `conf.level` in the `t.test` unless we want 0.95.

Elements of `t2` are printed below (after rearrangement)

```{r, echo = FALSE}
t2 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t2 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

- Invert signs of estimate and CI limits to get $\mu_{Statin} - \mu_{No}$.

# The Wilcoxon-Mann-Whitney Rank Sum procedure 

## Wilcoxon-Mann-Whitney Rank Sum Approach

The Wilcoxon-Mann-Whitney Rank Sum procedure requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.
3.  [Symmetry] The two populations are each symmetrically distributed, and as a result, we're comfortable estimating the shift in location (measured by the pseudo-medians) rather than a shift in means.

But it doesn't require:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

As mentioned, it doesn't really compare population means, but instead pseudo-medians.

## Wilcoxon-Mann-Whitney Rank Sum Approach

```{r}
wilcox.test(ldl ~ statin, data = dm431_cc, 
            conf.int = TRUE, conf.level = 0.90)
```

## Rank Sum test can also be tidied

```{r}
w3 <- tidy(wilcox.test(ldl ~ statin, data = dm431_cc, 
            conf.int = TRUE, conf.level = 0.90))
```

- Specify `conf.int` and `conf.level` in the `wilcox.test`.

Elements of `w3` are printed below (after rearrangement)

```{r, echo = FALSE}
w3 %>% 
  select(method, alternative, statistic) %>% 
  kable(digits = 2)

w3 %>% 
  select(estimate, conf.low, conf.high, p.value) %>% 
  kable(digits = 2)
```

- Invert signs of estimate and CI to describe shift from No to Statin.


## The Bootstrap

This bootstrap approach to comparing population means using two independent samples still requires:

1.	[Independence] The samples for the two groups are drawn independently.
2.	[Random Samples] The samples for each of the groups are drawn at random from the populations of interest.

but does not require either of the other two assumptions:

3.	[Normal Population] The two populations are each Normally distributed
4.	[Equal Variances] The population variances in the two groups being compared are the same.

The bootstrap procedure I use in R was adapted from Frank Harrell and colleagues. http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/BootstrapMeansSoftware

## The `bootdif` function

The procedure requires the definition of a function, which I have adapted a bit, called `bootdif`, which is part of the `Love-boost.R` script we loaded earlier.

As in our previous bootstrap procedures, we are sampling (with replacement) a series of many data sets (default: 2000).

- Here, we are building bootstrap samples based on the LDL levels in the two independent samples (statin users vs. non-users.) 
- For each bootstrap sample, we are calculating a mean difference between the two groups (statin vs. no statin.)
- We then determine the 2.5th and 97.5th percentile of the resulting distribution of mean differences (for a 95% confidence interval).  

## Using `bootdif` to compare mean(LDL) by statin

So, to compare LDL (our outcome) across the two levels of statin (our grouping factor) for the adult patients with diabetes in NE Ohio, run...

```{r}
set.seed(20201008)
boot4 <- dm431_cc %$% bootdif(ldl, statin, conf.level = 0.90)
boot4
```

- The two columns must be separated here with a comma rather than a tilde (`~`), and are specified using `$` notation. 
- This CI estimates $\mu_{Statin} - \mu_{NoStatin}$. Observe the listed sample mean difference for the necessary context. 
- If we change the `set.seed`, we'll get different endpoints for our CI.
- Note that we can infer the *p* value is above 0.10 from the CI. Why?


## Results for the LDL and Statin Study

Procedure     | *p* for $H_0: \mu_S = \mu_N$ | 90% CI for $\mu_S - \mu_N$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 2)` | (`r -1*round_half_up(t1$conf.high, 1)`, `r -1*round_half_up(t1$conf.low, 1)`)
Welch t   | `r round_half_up(t2$p.value, 2)` | (`r -1*round_half_up(t2$conf.high, 1)`, `r -1*round_half_up(t2$conf.low, 1)`)
Bootstrap   | *p* > 0.100 | (`r round_half_up(boot4[2],1)`, `r round_half_up(boot4[3],1)`)

Procedure     | *p* for $H_0: psmed_S = psmed_N$ | 90% CI for S - N shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 2)` | (`r -1*round_half_up(w3$conf.high, 2)`, `r -1*round_half_up(w3$conf.low, 2)`)

**Which method should we use?**

## Which Method Should We Use?

1. Plot the distributions of the two independent samples.
2. Does it seem reasonable to assume that **each** distribution (here, both `ldl` in statin users and `ldl` in non-users) follows an approximately Normal distribution?

- If Yes, Normal models seem fairly appropriate, then
  - use the indicator variable regression (pooled t test) if the sample sizes are nearly the same, or if the sample variances are reasonably similar
  - use the Welch's t test, otherwise (default `t.test` in R)
- If No, Normal models don't seem appropriate at all, then
  - compare means using the bootstrap via `bootdif`, or
  - compare pseudo-medians using the WMW rank sum test

What did we see in our `ldl` data? 

## LDL, within groups defined by `statin`

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431_cc, aes(x = statin_f, y = ldl, group = statin_f)) +
  geom_violin(aes(fill = statin_f)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431_cc, aes(sample = ldl, group = statin_f)) +
  geom_qq(aes(col = statin_f)) + geom_qq_line(col = "black") +
  facet_wrap(~ statin) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "LDL Cholesterol (mg/dl)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 1. Comparing LDL by Statin Use in our dm431 complete cases (n = 394)")
```

## Results for the LDL and Statin Study

Procedure     | *p* for $H_0: \mu_S = \mu_N$ | 90% CI for $\mu_S - \mu_N$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 2)` | (`r -1*round_half_up(t1$conf.high, 1)`, `r -1*round_half_up(t1$conf.low, 1)`)
Welch t   | `r round_half_up(t2$p.value, 2)` | (`r -1*round_half_up(t2$conf.high, 1)`, `r -1*round_half_up(t2$conf.low, 1)`)
Bootstrap   | *p* > 0.100 | (`r round_half_up(boot4[2],1)`, `r round_half_up(boot4[3],1)`)

Procedure     | *p* for $H_0: psmed_S = psmed_N$ | 90% CI for S - N shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 2)` | (`r -1*round_half_up(w3$conf.high, 2)`, `r -1*round_half_up(w3$conf.low, 2)`)

What conclusions should we draw, at $\alpha$ = 0.10?

# Example 3 (Comparing SBP by Sex) slides follow, for you to review on your own. It's very much like Example 1. The main difference is that we have no missing values in SBP or Sex in the `dm431` data.

## `dm431` Example 3. (Comparing SBP by Sex)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r, message = FALSE}
mosaic::favstats(sbp ~ sex, data = dm431) %>% kable(dig = 2)
```

- What is the outcome here? 
- What are the two exposure groups we are comparing?
- What are the sample means, $\bar{x}_F$ and $\bar{x}_M$?
- Point estimate of the difference in population means, $\mu_F - \mu_M$?

## DTDP for Example 3. (Comparing SBP by Sex)

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431, aes(x = sex, y = sbp, group = sex)) +
  geom_violin(aes(fill = sex)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431, aes(sample = sbp, group = sex)) +
  geom_qq(aes(col = sex)) + geom_qq_line(col = "black") +
  facet_wrap(~ sex) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "Systolic Blood Pressure (mm Hg)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 3. Comparing SBP by sex in our dm431 data")
```

## Linear Model for Example 3 (slide A)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r}
m1 <- lm(sbp ~ sex, data = dm431)

tidy(m1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

- What can we learn from this output? 
  - What is the sample mean `sbp` for females? 
  - What is the sample mean `sbp` for males?
  - The point estimate for $\mu_F - \mu_M$ is ...

## Linear Model for Example 3 (slide B)

Estimate the difference in population mean systolic blood pressure among females as compared to males.

```{r}
m1 <- lm(sbp ~ sex, data = dm431)

tidy(m1, conf.int = T, conf.level = 0.90) %>% kable(dig = 2)
```

```{r, echo = FALSE}
res <- tidy(m1, conf.int = T, conf.level = 0.90) %>% filter(term == "sexM")
```

- What can we learn from this output? 
  - The point estimate for $\mu_F - \mu_M$ is `r -1*round_half_up(res$estimate, 2)`
  - The 90% confidence interval for $\mu_F - \mu_M$ is (**`r -1*round_half_up(res$conf.high, 2)`**, **`r -1*round_half_up(res$conf.low, 2)`**)
  
## Building a Pooled t CI: Example 3

1. Best approach: use indicator variable regression
2. Also: direct call to t test with pooled variance estimate

```{r}
t.test(sbp ~ sex, data = dm431, alt = "two.sided", mu = 0,
       var.equal = TRUE, conf.level = 0.90)
```

## `t` test can be tidied

```{r}
t1 <- tidy(t.test(sbp ~ sex, data = dm431, 
                  var.equal = TRUE, conf.level = 0.90))
```

- `conf.level` must be specified to `t.test`. Otherwise, it uses 0.95.

Elements of `t1` are printed below (after rearrangement)

```{r, echo = FALSE}
t1 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t1 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

## Building the Welch t CI: Example 3

- Sensible approach when assuming Normal populations is OK, but we don't want to assume the two populations have the same variance (as pooled t requires)

```{r}
t.test(sbp ~ sex, data = dm431, alt = "two.sided", mu = 0,
       conf.level = 0.90)
```

## Welch `t` test can also be tidied

```{r}
t2 <- tidy(t.test(sbp ~ sex, data = dm431, 
                  conf.level = 0.90))
```

- We must specify `conf.level` in the `t.test` unless we want 0.95.

Elements of `t2` are printed below (after rearrangement)

```{r, echo = FALSE}
t2 %>% 
  select(method, alternative, estimate1, estimate2) %>% 
  kable(digits = 2)

t2 %>% 
  select(estimate, conf.low, conf.high, statistic, parameter, p.value) %>% 
  kable(digits = 2)
```

## Wilcoxon-Mann-Whitney Rank Sum: Example 3

```{r}
wilcox.test(sbp ~ sex, data = dm431, 
            conf.int = TRUE, conf.level = 0.90)
```

## Rank Sum test can also be tidied

```{r}
w3 <- tidy(wilcox.test(sbp ~ sex, data = dm431, 
            conf.int = TRUE, conf.level = 0.90))
```

- Specify `conf.int` and `conf.level` in the `wilcox.test`.

Elements of `w3` are printed below (after rearrangement)

```{r, echo = FALSE}
w3 %>% 
  select(method, alternative, statistic) %>% 
  kable(digits = 2)

w3 %>% 
  select(estimate, conf.low, conf.high, p.value) %>% 
  kable(digits = 2)
```

## Using `bootdif` to compare mean(SBP) by Sex

So, to compare systolic BP (our outcome) across the two levels of sex (our grouping factor) for the adult patients with diabetes in NE Ohio, run...

```{r}
set.seed(431431)
boot4 <- dm431 %$% bootdif(sbp, sex, conf.level = 0.90)
boot4
```

- This CI estimates $\mu_{M} - \mu_{F}$: observe the listed sample mean difference for the necessary context. 
- Invert the signs to estimate $\mu_{F} - \mu_{M}$.
- Again the *p* value must be larger than 0.10 since 0 is in the 90% CI.

## Results for the SBP and Sex Study

Procedure     | *p* for $H_0: \mu_F = \mu_M$ | 90% CI for $\mu_F - \mu_M$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 4)` | (`r round_half_up(t1$conf.low, 2)`, `r round_half_up(t1$conf.high, 2)`)
Welch t   | `r round_half_up(t2$p.value, 4)` | (`r round_half_up(t2$conf.low, 2)`, `r round_half_up(t2$conf.high, 2)`)
Bootstrap   | *p* > 0.100 | (`r -1*round_half_up(boot4[3],2)`, `r -1*round_half_up(boot4[2],2)`)

Procedure     | *p* for $H_0: psmed_F = psmed_M$ | 90% CI for F - M shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 4)` | (`r round_half_up(w3$conf.low, 2)`, `r round_half_up(w3$conf.high, 2)`)

**Which method should we use?**

## Systolic BP, within groups defined by sex

```{r, fig.height = 6, echo = FALSE}
p1 <- ggplot(dm431, aes(x = sex, y = sbp, group = sex)) +
  geom_violin(aes(fill = sex)) +
  geom_boxplot(width = 0.3, outlier.size = 3) +
  coord_flip() + guides(fill = FALSE) 

p2 <- ggplot(dm431, aes(sample = sbp, group = sex)) +
  geom_qq(aes(col = sex)) + geom_qq_line(col = "black") +
  facet_wrap(~ sex) + guides(col = FALSE) +
  theme(aspect.ratio = 1) + labs(y = "Systolic Blood Pressure (mm Hg)")

p1 / p2 + plot_layout(heights = c(1,3)) + 
  plot_annotation(title = "Example 3. Comparing SBP by sex in our dm431 data")
```

## Results for the SBP and Sex Study

Procedure     | *p* for $H_0: \mu_F = \mu_M$ | 90% CI for $\mu_F - \mu_M$
:-----------: | --------------------: | :------------------------:
Pooled t  | `r round_half_up(t1$p.value, 4)` | (`r round_half_up(t1$conf.low, 2)`, `r round_half_up(t1$conf.high, 2)`)
Welch t   | `r round_half_up(t2$p.value, 4)` | (`r round_half_up(t2$conf.low, 2)`, `r round_half_up(t2$conf.high, 2)`)
Bootstrap   | *p* > 0.100 | (`r -1*round_half_up(boot4[3],2)`, `r -1*round_half_up(boot4[2],2)`)

Procedure     | *p* for $H_0: psmed_F = psmed_M$ | 90% CI for F - M shift
:-----------: | --------------------: | :------------------------:
Rank Sum  | `r round_half_up(w3$p.value, 4)` | (`r round_half_up(w3$conf.low, 2)`, `r round_half_up(w3$conf.high, 2)`)


What conclusions should we draw, at $\alpha$ = 0.10?

